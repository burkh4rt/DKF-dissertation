% !TEX root = ../discriminative_filtering.tex

%Dynamic state space models provide a statistical framework for relating a sequence of hidden states and their corresponding measurements.  Given a state model for the current latent state given the previous one and a measurement model for the current observation given the current latent state, a Bayesian filtering algorithm calculates or approximates the posterior distribution of the current latent state given all observations up to the present time.  The Kalman filter that specifies linear, Gaussian state and measurement models allowed the Apollo lunar module to infer its location from earth-based radar measurements and land safely on the moon.

% Given a sequence of hidden states and corresponding observed measurements, Bayesian filtering provides a principled framework for inferring the distribution of the current hidden state given all measurements up to the current time.  

Given a stationary state-space model that relates a sequence of hidden states and corresponding measurements or observations, Bayesian filtering provides a principled statistical framework for inferring the posterior distribution of the current state given all measurements up to the present time.  For example, the Apollo lunar module implemented a Kalman filter to infer its location from a sequence of earth-based radar measurements and land safely on the moon.  

To perform Bayesian filtering, we require a measurement model that describes the conditional distribution of each observation given state.  The Kalman filter takes this measurement model to be linear, Gaussian.  Here we show how a nonlinear, Gaussian approximation to the distribution of state given observation can be used in conjunction with Bayes' rule to build a nonlinear, non-Gaussian measurement model.  The resulting approach, called the Discriminative Kalman Filter (DKF), retains fast closed-form updates for the posterior.  We argue there are many cases where the distribution of state given measurement is better-approximated as Gaussian, especially when the dimensionality of measurements far exceeds that of states and the Bernstein--von Mises theorem applies. Online neural decoding for brain-computer interfaces provides a motivating example, where filtering incorporates increasingly detailed measurements of neural activity to provide users control over external devices.  Within the BrainGate2 clinical trial, the DKF successfully enabled three volunteers with quadriplegia to control an on-screen cursor in real-time using mental imagery alone.  Participant ``T9'' used the DKF to type out messages on a tablet PC.

Nonstationarities, or changes to the statistical relationship between states and measurements that occur after model training, pose a significant challenge to effective filtering.  In brain-computer interfaces, one common type of nonstationarity results from wonkiness or dropout of a single neuron.  We show how a robust measurement model can be used within the DKF framework to effectively ignore large changes in the behavior of a single neuron.  At BrainGate2, a successful online human neural decoding experiment validated this approach against the commonly-used Kalman filter. 

%  designed to mimic nonstationarities

%The Kalman filter is a simple and efficient algorithm for computing the posterior distribution of latent states in a linear, Gaussian, state space model when the observation model is also linear and Gaussian. Extensions to the Kalman filter have been proposed that incorporate linear approximations to nonlinear models for $p(\text{observation}|\text{state})$ such as the extended Kalman filter (EKF) and the unscented Kalman filter (UKF). We argue that there are many cases in which a model for $p(\text{state}|\text{observation})$ proves both easier to learn and more accurate for latent state estimation.
%
%Approximating $p(\text{state}|\text{observation})$ as Gaussian leads to a new filtering algorithm---the Discriminative Kalman filter---that can perform well even on models where $p(\text{observation}|\text{state})$ is highly nonlinear and/or non-Gaussian. The approximation is based on the Bernstein--von Mises theorem, it improves as the dimensionality of the observations increases, and it maintains much of the simplicity and efficiency of the original Kalman filter. 
%
%In situations where an unknown observation model must be learned from fully-supervised training data prior to filtering, off-the-shelf nonlinear and/or nonparametric regression techniques can be used to learn the components needed for the Discriminative Kalman Filter (DKF). Density estimation is not required. The DKF is validated on both real and synthetic datasets. It has been successfully implemented in an intracortical brain computer interface, providing people with closed-loop neural cursor control as part of the ongoing BrainGate2 clinical trial.  Furthermore, it has been used in conjunction with a robust model for $p(\text{state}|\text{observation})$ to mitigate certain types of commonly-encountered nonstationarities in human neural filtering.

